本部分，将对基于图文信息融合的多模态意图检测方法进行训练与测试，在公开数据集上验证模型的性能，并与基线模型进行对比实验，完成自身模型模块的消融实验。


本章所有的对比实验与消融实验均在搭载2张Tesla T4显卡的服务器上运行，服务器内存为56GB，显存一共为32GB，系统为Ubuntu，实验代码运行环境采用Docker镜像配置，其中编程语言为Python 3.8，深度学习框架采用PyTorch 2.0。具体的实验环境参数如表3-1所示。

表


实验使用的是最新公开的多模态意图识别数据集(MIntRec)【1】，该数据是由清华大学于2022年整理发布的。MIntRec是一个多模态意图识别数据集，主要用于在真实多模态场景中进行意图识别，也是目前第一个用于现实世界多模态场景意图识别的基准数据集。数据来源于美剧Superstore，从中筛选了2224条高质量的多模态意图样本。每条样本包含文本、图片和音频三种模态信息以及多模态意图标签。该数据集结合多模态场景构建了新的层次化意图体系，包含两个粗粒度和20个细粒度意图类别。Inspired by human intention philosophy and goal-oriented intentions in artificial intelligence research, the data is categorized two coarse-grained intent categories: "Express emotions or attitudes" and "Achieve goals". "Express emotions and attitudes" 包含11个细粒度意图类别： Complain, Praise, Apologize, Thank, Criticize, Care, Agree, Taunt, Flaunt, Oppose, Joke. "Achieve goals" are classified into nine categories: Inform, Advise, Arrange, Introduce, Comfort, Leave, Prevent, Greet, Ask for help. The statistics of these datasets are
given in Table 2, we split training, validation, and testing sets in 6:2:2. The detailed statistics are shown in Table 3


表




本实验中使用准确率（Accuracy）、精度(P)、召回率(R)和F1-score作为模型的性能评价指标，准确率（Accuracy）是衡量模型精确度最直观的指标，其计算方法如公式(3-7)所示。

公式1

其中TP为预测正样本，实际也为正样本的数量，TN为预测负样本，实际也为负样本的数量，FP为预测为正样本，实际为负样本的数量，FN为预测为负样本，实际为正样本的数量。




与准确率类似地，查准率 Precision 用于衡量在模型的预测结果为正的样本
中，被预测正确的正样本所占的比例。其计算公式为：

公式2

除此之外，召回率 Recall 表示模型正确预测的正样本在所有正样本中所占的
比例，且召回率和查准率是一对矛盾的指标，当召回率高的时候，查准率一般很
低；查准率高时，召回率一般很低。其计算公式为：

公式3

然而，对于正负样本数量不平衡的数据集，以上评价指标均存在一定程度的
缺陷，因为即使分类器将所有样本都预测为数量较多的类别也能得到较高的准确
率和查准率，但这样的分类器实际上没有任何作用。对于正负样本数量不平衡的
数据，更加合理的评价指标为 F1 得分，其计算公式为：

公式4

其中𝑃为查准率 Precision；𝑅为召回率 Recall。


F1-score是一种二分类指标，用来评估不均衡数据的模型精度，可以看作是模
型精确率和召回率的一种加权平均。而在数据样本不平衡的多分类问题中，需采
用Micro-F1或Macro-F1指标来评价性能，We report the macro score over all classes for the
last three metrics. The higher values indicate better performance
of all metrics.







是一个ResNet50的改进版本，文本编码器是一个使用8头注意力、12层512维度的
Transformer。顺序注意模块也是一个8头注意力、12层512维度的Transformer。
在所有实验中，batch-size设置为128，Adam的权重衰减设置为1e-4，学习率设置
为1e-5。
由于包含预训练的权重和随机初始化的权重的混合，而不
是端到端训练模型，这可能会导致某些模块的欠拟合或过拟合，为此，采用多阶
段训练策略来训练整个模型，具体分为以下两个阶段：
（1）模块训练阶段。此时，图像编码器和文本编码器的权重被冻结，训练特
征融合增强模块、跨模态注意模块和MLP分类器中的参数。

实验细节

在实验中，本章使用 Pytorch和HuggingFace Transformers框架来实现baseline和本研究所提出的模型。

在模型的特征提取部分，使用clip(clip-vit-base-patch16)同时提取文本和图像特征，其中图像编码器和文本编码器分别使用ViT-B/16和基于自注意力机制的transformer结构。

The ViT-B/32 model uses a patch size of 32x32 pixels to extract image features, which means that the input image is divided into 32x32 non-overlapping patches. Each patch is flattened into a 2D vector and fed into the transformer encoder. The number of patches is then reduced by a factor of 96 to obtain a sequence of image features.

The transformer encoder consists of 12 self-attention layers and 12 feed-forward layers. Each layer has a hidden dimension of 768, and the number of heads in the self-attention layer is set to 12. The output of the last feed-forward layer is then flattened into a vector, which is used as the input for the final classification layer.


跨模态融合阶段，使用8头交叉注意力，6层512维度的Transformer。在分类阶段，受限于数据集大小，为避免出现过拟合现象，构建了2层MLP和一个softmax层简单分类网络，softmax层维度和意图标签数量保持一致，每个值代表所属对应标签的概率。在模型的训练阶段，采用预训练的CLIP权重作为本章模型中图像编码器和文本编码器的初始权重，随机初始化跨模态注意模块和MLP分类器中的权重。其它主要的超参如表所示，超参设置主要通过观察结果和基于前人先验知识确定。

表






MAG-BERT. Rahman et al. [1] integrated two nonverbal
modalities into BERT with an additional multimodal adaptation gate
(MAG) module. MAG can produce a position shift in the semantic
space adaptive to acoustic and visual information. It can be flexibly
placed between layers of BERT to receive inputs from nonverbal
modalities.


MulT. The Multimodal Transformer (MulT) [46] is an end-toend method to deal with non-aligned multimodal sequences. It extends the vanilla Transformer [47] to the cross-modal Transformer
with the pairwise inter-modal attention mechanism, which helps
to capture the adaptation knowledge between different modalities
in the latent space.


Baselines

MulT. The Multimodal Transformer (MulT) [46] is an end-toend method to address the challenge of processing and understanding information from multiple modalities that may not be temporally synchronized or aligned, MulT extends the Transformer architecture to capture the adaptation knowledge between different modalities in the latent space.


提出了一个多模态适应门结构（MAG），这是一种基于BERT模型的改进版本，允许模型输入非文本模态，It can be flexibly
placed between layers of BERT. 不同模态的输入会影响词汇的意义，进而影响向量在语义空间的位置，MAG can produce a position shift 重新计算向量在语义空间的新位置。


Trans_TAV。该模型是一种相对简单的多模态学习方法，which utilizes an early fusion approach for combining features from different modalities. 该方法可以使用BERT提取文本信息，使用Wav2vec和Faster R-CNN分别提取音频和视
频信息。

BERT（Bidirectional Encoder Representations from Transformers）是一种预训练的自然语言处理（NLP）模型，它采用了Transformer架构，并在大规模文本语料库上进行了预训练，以学习通用的语言表示。

ResNet-50是一种针对图片的预训练模型，主要用于图像分类任务，也经常被用作迁移学习的基础模型，用于处理各种计算机视觉任务。

为了验证本方法的有效性，选择了三个主流的多模态学习模型和两个主流的单模态学习模型与本研究所提出的方法进行比较：


其中MulT，MAG-BERT和Trans_TAV是多模态学习的代表模型，前两者基于注意力机制，综合考虑了不同模态特征的表征、对齐和融合，相对于Trans_TAV更为复杂和先进，具备更好的多模态学习能力，而Trans_TAV实现方式比较简单，但在特征表示和融合方面存在不足，是早期传统多模态学习方法的典型代表。BERT和ResNet-50是单模态模型，分别用于处理文本和图片，也是NLP和CV领域的代表性模型。通过和以上五个代表性模型的对比可以有效评估本研究所提出的基于多模态预训练模型和跨模态注意力机制的多模态学习方法在意图检测上的性能。在实验过程中，基准模型的参数设置主要参考默认值，为保证使用的模态统一，所有模型仅使用图片和文本模态。


表1展示整体的对比实验结果。通过实验结果，我们可以得到以下实验结论。


首先，从整体指标来看，本研究提出的CLIF_CMA多模态学习方法和其它代表性baseline模型相比，在意图检测数据集上表现了出色的性能，从而验证了该方法的有效性。
其次，从输入模态来看，多模态模型的结果普遍优于单模态模型的结果，因为随着输入模态的增加可以提供更多的有效信息，这表明了融合多模态信息进行意图检测的必要性。此外，就单个模态而言，文本模态取得了最佳的性能，这表明文本在意图检测中包含了比图片更多的意图识别信息，而且得益于大规模预训练语言模型的发展，文本可以通过迁移学习方法获得更好的语义表示。单独使用图像模态效果最差，这可能是因为图像中的特征比较分散，噪声较多，导致模型很难从图像中获取到和意图相关的有效特征。
最后，从多模态模型来看，Trans_TAV模型效果最差，这可能是因为将特征直接拼接起来，或仅仅使用简单的加权求和方式融合单模态特征难以有效利用多模态之间的互补性，这也说明在多模态学习中，需要设计合理的多模态融合方法，才能有效利用多模态信息，从而提高模型的性能。MulT性能优于Trans_TAV，因为该模型结构能够捕捉多模态序列之间的交互关系,但和MAG-BERT模型相比，缺乏考虑到多模态数据之间的语义一致性。



不同clip对比实验

CLIP多模态预训练模型包含文本和图像编码器，文本编码器主要是使用基于注意力机制的transformer结构，根据图像编码器的不同，OpenAI提供了两大类预训练CLIP模型，分别是基于RNN结构的ResNet系列和基于transformer结构的ViT系列，ResNet主要包括RN50x16和RN50x64两种，x16 and x64 implie a scaling factor applied to the number of channels (or filters) in each layer。ViT主要包括ViT-B/32和ViT-B/16两种，32 and 16 Refer to the patch size used in the input images. 为了验证不同编码器对性能的影响，使用以上四种编码器进行对比实验，实验结果如表所示：

通过实验发现，ResNet和ViT系列表现相近，两者都是计算机视觉领域主流的预训练模型。相较于ViT-B-32,ViT-B-16的准确率和F1值分别提高了0.80和0.99百分点,性能表现最好，这是由于块的大小对模型性能产生影响,一般情况，更小的块可以捕捉到更细粒度的图像特征，但实际效果主要depends on the characteristics of specific task and dataset，由于该数据集的规模不大、多样性不足,难以有效判断2个模型的优劣。 



消融实验

为验证本研究中各模块对基于图文的意图检测的性能提升效果,分别针对不同类型数据，特征表征方法和融合方法三部分在相同数据集上进行消融实验研究。实验结果如表所示，其中“-t”表示移除文本数据，使用空字符串代替，“-p”表示移除图片数据，使用空白图片代替， “-CLIP”表示移除CLIP模块，使用Bert和ResNet代替分别提取文本和图片特征，“-CMA”表示移除交叉注意力特征融合模块，使用concat方式融合特征，


从前两行可以看出，去除文本后，仅使用图片数据的效果最差，准确率和F1得分分别只有0.2和0.4，这说明文本特征在意图识别中起着重要作用，而图像信息的作用主要是对文本信息的扩展，仅依赖于视觉特征的意图检测是难以投入到现实使用的。与之相比，仅使用文本信息数据进行意图检测时的准确率超过了0.7，在性能上并没有比多模态基线模型落后太多，这说明本研究使用的文本特征和用户想要表达的想法相关度较高。从第三行可以看出，使用Bert和ResNet代替clip模型后效果有所下降，这种多模态学习方式和Trans_TAV模型类似，在特征提取和表征时没有考虑模态之间的相关性，在后续阶段难以准确地融合不同输入所表达的信息。从第四行可以看出，使用简单的拼接方式融合多模态特征，在性能上甚至低于仅仅使用文本模态的BERT模型。这就意味着，在文本信息的基础上引入视觉信息虽然使得模型拥有了更加丰富的特征，但也产生了非常多的冗余信息甚至是噪声，单纯依靠对多模态信息的空间操作进行融合的方法难以直接获取两种模态内在的相互作用。所以，如果不能妥善处理额外模态的信息，反而会对模型的性能产生反作用。


case分析

我们使用混淆矩阵直观展示了每个意图的预测效果，以进一步分析测试数据中预测错误的案例，如图1所示，其中横轴和纵轴分别代表预测标签和真实标签，并用颜色表示预测概率，对角线是预测标签等于真实标签，颜色越深表示该意图下准确率越高。

整体来看，该模型在大多数类别中表现出了较高的准确率，但在不同类别的意图上性能也有明显区别，有的意图表达模式相对固定，内容比较具体，例如表扬、感谢、道歉、同意和问候等，模型在这几类上表现出了更优秀的性能。然而，在一些复杂场景下，例如炫耀、告知和嘲讽等，模型表现一般，这可能是因为这些意图的表达方式比较多样化，而且内容比较抽象，要合理地推断出说话人的真实意图，可能需要音频和动作等额外模态信息。从混淆矩阵中可以看出模型容易混淆告知与安排、抱怨与反对，这些类别本身具有很高的相似性，容易造成误判，对于这些意图可能依赖上下文信息才能实现更准确地判断。这些问题也表明复杂场景下的多模态意图检测任务仍有巨大的改进空间。


结论

本文主要从图像和文本两种不同的模态出发，探索了图文信息融合技术在多媒体意图检测中的应用。首先，本研究设计了基于多模态预训练模型和交叉注意力机制的多模态学习方法，以实现更准确的意图检测。为了更好地综合利用这两种不同模态的信息，将意图检测任务分为多模态特征表征和融合两部分。在特征表征部分，我们提出使用CLIP多模态预训练同时提取文本和图像特征，并在微调后自动实现对齐，赋予了模型在少量样本下的学习能力。在融合部分基于交叉注意力机制，以有效地融合不同模型的信息。然后，通过和基线模型在相同数据集上进行对比实验证明了该模型的有效性。接着通过消融实验,验证各模块的有效性。最后分析了模型在不同意图标签上的具体表现。由于数据资源和硬件设备的限制，本研究仍有许多不足之处，有进一步改进的空间。本文仅使用了MIntRec数据集的文本部分和视觉部分，后续研究将加入视频中的音频模态,以确保数据的完整性,进一步提升多模态意图检测的准确率和泛化能力。同时，在多媒体中普遍存在缺少部分模态数据的情况，如何解决输入数据中模态缺失问题对于实际的模型落地具有重要意义。











































































